\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{geometry}

% Page Setup
\geometry{a4paper, margin=1in}

% Title Section
\title{\textbf{Eigenvalue Computation of Complex Matrices}}
% Using Householder Transformations, Givens Rotations, and Gram-Schmidt Algorithm}
\author{Agamjot Singh\\EE24BTECH11002\\IIT Hyderabad}
\date{\today}

\newcommand{\vecb}[1]{\mathbf{#1}}
\newcommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\newcommand{\defmat}[2]{#1\in\mathbb{C}^{#2\times#2}}
\newcommand{\defvec}[2]{\vecb{#1}\in\mathbb{C}^{#2}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\maketitle

\begin{abstract}
This report presents an efficient algorithm for finding the eigenvalues of matrices with complex entries using a combination of Householder transformations and QR decomposition (via Givens rotation or Gram-Schmidt algorithm). The implementation is in C, and we discuss the theory, the approach taken, and provide numerical examples to demonstrate the effectiveness of the method.
\end{abstract}

\section{Introduction}
Finding eigenvalues of complex matrices is a critical task in various scientific and engineering applications. This report explores an efficient approach using a combination of:
\begin{itemize}
    \item \textbf{Householder Transformations:} to reduce the matrix to Hessenberg form.
    \item \textbf{Givens Rotations/Gram-Schmidt:} for QR decomposition of the Hessenberg matrix.
\end{itemize}

\section{Theory}
We start off by defining some basic concepts that we will be using throughout:
\subsection{Eigenvectors and Eigenvalues}
When we multiply a vector $\defvec{v}{m}$, $\vecb{v} \neq \vecb{0}$ by a matrix $\defmat{A}{m}$ we get a resultant vector $\vecb{x} = A\vecb{v}$, $\defvec{x}{m}$. This transformation rotates, stretches, or shears the vector $\vecb{v}$. Now we choose the vector $\vecb{v}$ such that this linear transformation only stretches, with no rotation or shear.
This chosen vector is the \textbf{eigenvector} of the matrix $A$. The corresponding \textbf{eigenvalue} is defined as the factor by which the eigenvector has been stretched.
\newline
Mathematically speaking,
\begin{align}
\label{eigen:1}
    A\vecb{v} = \lambda\vecb{v}
\end{align}
where $\vecb{v}$ is the eigenvector and $\lambda \in \mathbb{C}$ is the corresponding eigenvalue.
\newline

Equation \brak{\ref{eigen:1}} can also be stated as,
\begin{align}
\label{eigen:2}
   \brak{A - \lambda I} \vecb{v} = \vecb{0} 
\end{align}
where $I \in \mathbb{C}^{m \times m}$ is the identity matrix of order $m$.
\newline

Equation \brak{\ref{eigen:2}} has non zero solution $\vecb{v}$ if and only if, 
\begin{align}
\label{eigen:3}
    \text{det}\brak{A - \lambda I} = 0
\end{align}

\subsection{Similarity Transformation}
A similarity transformation on a matrix $\defmat{A}{m}$ with transformation matrix $\defmat{A^{\prime}}{m}$ is given by,
\begin{align}
    A^{\prime} = X^{\ast} A X
\end{align}
where $X$ is a unitary matrix and $X^{\ast}$ is the conjugate transpose of $X$.
Note that, for $X$ to be unitary, it satisfies,
\begin{align}
    \label{sim:2}
    X^{\ast}X = XX^{\ast} = I
\end{align}

We will now show that a similarity transformation preserves the eigenvalues of any matrix.
\newline
Say $\lambda^{\prime} \in \mathbb{C}$ is an eigenvalue of the matrix $A^{\prime}$, hence by equation \brak{\ref{eigen:3}} and \brak{\ref{sim:2}},
\begin{align}
    \text{det}\brak{A^{\prime} - \lambda^{\prime} I} &= 0\\
    \text{det}\brak{X^{\ast}AX - \lambda^{\prime} X^{\ast}X} &= 0\\
    \text{det}\brak{X^{\ast}\brak{A - \lambda^{\prime}I} X} &= 0\\
    \label{sim:3} \implies \text{det}\brak{X^{\ast}} \text{det}\brak{A - \lambda^{\prime}I} \text{det}\brak{X} &= 0\\
\end{align}
From equation \brak{\ref{sim:2}}, $X$ is invertible with $X^{-1} = X^{\ast}$. Hence $\text{det} \brak{X} = \frac{1}{\text{det} \brak{X^{\ast}}}\neq 0$.
\newline
Using this fact in equation \brak{\ref{sim:3}}, we get,
\begin{align}
    \text{det}\brak{A - \lambda^{\prime} I} = 0
\end{align}

This proves that eigenvalues of $A^{\prime}$ are exactly same as the eigenvalues of $A$. Similarily, it can also be proved that eigenvalues of $A$ are same as that of $A^{\prime}$.
\newline
Similarily transformations are the basic building block of computing eigenvalues efficiently which will be extensively used in this implementation.

\subsection{QR decomposition} \label{qrth}
$QR$ decomposition is a decomposition of a matrix $\defmat{A}{m}$ into a product $A = QR$, such that $\defmat{Q}{m}$ is an square unitary matrix with all of its columns having unit norm and $\defmat{R}{m}$ is an upper triangular matrix. 
\newline
\newline
Geometrically speaking, the matrix $A$ can be thought of as a set of vectors \brak{\text{columns of } A} in the $m$-dimensional space.
The matrix $Q$ represents an orthonormal basis for the column space of $A$. It "replaces" the original vectors of $A$ with a new set of orthonormal vectors.
The matrix $R$ provides the coefficients needed to express the original vectors of $A$ as linear combinations of the new orthonormal vectors in $Q$.

\section{Algorithm Overview}
We employ a multi-step approach to find the eigenvalues of a complex matrix $\defmat{A}{m}$:

\subsection{The Basic QR Algorithm - Schur Decomposition}
In this algorithm, a sequence of matrices $\cbrak{A_k}$, $\defmat{A_k}{m}$ is generated iteratively, converging towards an upper triangular matrix. Under specific conditions, the convergence of off-diagonal elements follows a rate based on the ratio of eigenvalues.

As discussed in section $\ref{qrth}$, any matrix $A_k$ can be expressed as,
\begin{align}
    A_k = Q_k R_k
\end{align}
where $Q_k^{\ast}Q_k = I$.
\newline
Now we take $A_{k+1}$ such that,
\begin{align}
    A_{k + 1} = R_k Q_k = Q_k^{\ast} A_k Q_k
\end{align}
We can observe that the above transformation is a similarity transformation. Hence the eigenvalues of any matrix in the sequence $\cbrak{A_k}$ are identical.
\newline
The matrix sequence $\cbrak{A_k}$ converges \brak{\text{although very slowly in its current form}} towards an upper triangular matrix, with its eigenvalues as diagonal elements.
This algorithm computes an upper triangular matrix $T$ and a unitary matrix $U$ such that $A = UTU^{\ast}$ is the Schur decomposition of $A$.

\begin{align}
    A_k = Q_k^{\ast} A_{k - 1} Q_k = Q_k^{\ast} Q_{k - 1}^{\ast} A_{k - 2} Q_{k - 1} Q_k = \dots = Q_k^{\ast}\dots Q_1^{\ast} A Q_1 \dots Q_k
\end{align}
We can observe that $U_k = Q_1 Q_2 Q_3 \dots Q_k$, and $U = U_{\infty}$.
Also note that $T = A_{\infty}$.

Let the eigen values of $A$ be $\lambda_1$, $\lambda_2$, $\lambda_3 \dots \lambda_m$, such that
\begin{align}
    \abs{\lambda_1} \geq \abs{\lambda_2} \geq \abs{\lambda_3} \dots \geq \abs{\lambda_m}
\end{align}
We state, without proof, that the subdiagonal elements of $A$ converge like
\begin{align}
    \abs{a_{ij}^{\brak{k}}} = O\brak{\abs{\frac{\lambda_i}{\lambda_j}}^k}, i > j
\end{align}

\subsection{Householder Transformations}
As stated before, the vanilla $QR$ algorithm converges very slowly without any tweaking involved.
One such tweak is reducing the matrix to a hessenberg matrix form using similarity transforms and then apply the $QR$ algorithm. 
This is achieved by Householder Transformations on the original matrix to convert it into hessenberg form.
\newline
Note that a Hessenberg matrix is of the form,
\begin{align}
H = \begin{bmatrix}
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & 0 & \times & \times & \times\\
0 & 0 & 0 & \times & \times
\end{bmatrix}   
\end{align}
\subsubsection{Householder reflectors}
A Householder reflector matrix is of the form
\begin{align}
    P = I - 2\vecb{u}\vecb{u}^{\ast}
\end{align}
where $\defmat{P}{m}$ and $\defvec{u}{m}$, $\norm{\vecb{u}} = 1$.

Note that Householder reflectors are Hermitian, i.e. $P^{\ast} = P$ and $P^2 = I$. As $P^{\ast}P = I$, we also come to the conclusion
that $P$ is a unitary matrix.
\newline
We want the Householder reflector to transform any vector $\defvec{x}{m}$ to a multiple of $\vecb{e_1}$, where $\vecb{e_n}$ is the impulse vector with $n = 1$.
\begin{align}
\label{hhr:1}
    P\vecb{x} = \vecb{x} - 2\vecb{u}\brak{\vecb{u^{\ast}}\vecb{x}} = \alpha \vecb{e_1}
\end{align}
Since $P$ is unitary, $\norm{P\vecb{x}} = \norm{\vecb{x}}$.
\newline
Hence by taking norm on both sides on equation \brak{\ref{hhr:1}}, we get $\abs{\alpha} = \norm{\vecb{x}}$. Therefore, $\alpha = \rho\norm{\vecb{x}}$, where $\rho \in \mathbb{C}$, $\abs{\rho} = 1$. 
\newline
By rearranging equation \brak{\ref{hhr:1}},
\begin{align}
    \vecb{x} - \rho\norm{\vecb{x}}\vecb{e_1} = 2\vecb{u}\brak{\vecb{u^{\ast}}\vecb{x}}
\end{align}
As $\vecb{u}$ is unit norm,
\begin{align}
\label{hhr:2}
    \vecb{u} = \frac{\vecb{x} - \rho\norm{\vecb{x}}\vecb{e_1}}{\norm{\vecb{x} - \rho\norm{\vecb{x}}\vecb{e_1}}} = \frac{1}{\norm{\vecb{x} - \rho\norm{\vecb{x}}\vecb{e_1}}} \begin{pmatrix} x_1 - \rho\norm{\vecb{x}}\\x_2\\\vdots\\x_n\end{pmatrix}
\end{align}
Selection of $\rho$ is flexible as long as $\abs{\rho} = 1$. To ease out the process, we take $\rho = \frac{x_1}{\abs{x_1}}$, $x_1 \neq 0$. If $x_1 = 0$, we take $\rho = 0$.

\subsubsection{Reduction to Hessenberg Form using Householder reflectors}
Consider the initial matrix $\defmat{A}{5}$ for the sake of explanation:

\begin{align}
A = \begin{bmatrix}
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times
\end{bmatrix}
\end{align}

Let $P_1$ have the structure,
\begin{align}
P_1 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\    
0 & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & \times & \times & \times & \times
\end{bmatrix} = \begin{bmatrix}
1 & \vecb{0}^{\ast}\\    
\vecb{0} & I_4 - 2\vecb{u_1}\vecb{u_1^{\ast}}
\end{bmatrix}
\end{align}
and let $P_2$ have the structure
\begin{align}
P_2 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\    
0 & 1 & 0 & 0 & 0\\
0 & 0 & \times & \times & \times\\
0 & 0 & \times & \times & \times\\
0 & 0 & \times & \times & \times
\end{bmatrix} = \begin{bmatrix}
1 & 0 & \vecb{0}^{\ast}\\    
0 & 1 & \vecb{0}^{\ast}\\
\vecb{0} & \vecb{0} & I_3 - 2\vecb{u_2}\vecb{u_2^{\ast}}
\end{bmatrix}
\end{align}

Similarily $P_3$, $P_4 \dots$ can also be found using relevant $\vecb{u}$.
The Householder vector $\vecb{u_1}$ is found taking $\vecb{x}$ in equation \brak{\ref{hhr:2}} as $\begin{pmatrix}a_{21}\\a_{31}\\a_{41}\\a_{51}\end{pmatrix}$.
\newline
Similary $\vecb{u_2}$, $\vec{u_3} \dots$ can also be found taking elements below the diagonal of each column as $\vecb{x}$.
\newline
The multiplication of $P_1$ from the left inserts the desired zeros in the first column of $A$. The multiplication from the right is necessary in order to have similarity and preserve eigen values.
Because of the structure of $P_1$ the first column of $P_1 A$ is not affected.
The reduction happens in the following way:
\begin{align}
P_1 A P_1 = \begin{bmatrix}
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & \times & \times & \times & \times
\end{bmatrix} 
\end{align}

\begin{align}
P_2 \brak{P_1 A P_1} P_2 = \begin{bmatrix}
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & 0 & \times & \times & \times\\
0 & 0 & \times & \times & \times
\end{bmatrix}
\end{align}

\begin{align}
P_3 \brak{P_2 P_1 A P_1 P_2} P_3 = \begin{bmatrix}
\times & \times & \times & \times & \times\\
\times & \times & \times & \times & \times\\
0 & \times & \times & \times & \times\\
0 & 0 & \times & \times & \times\\
0 & 0 & 0 & \times & \times
\end{bmatrix} = H
\end{align}

which $H$ is the transformed matrix in the Hessenberg form.


\subsection{QR Decomposition using Givens Rotations}
Now that we have reached the hessenberg form, we have to perform the $QR$ algorithm to converge it into an upper triangular matrix.
For $QR$ decomposition, i.e finding both $Q$ and $R$ such that $H = QR$, we use Givens Rotations to zero out subdiagonal elements to find upper triangular R, and then calculate $\tilde{H} = RQ$.

The Givens rotation matrix $G\brak{i, j, c, s}$ is defined by
\begin{align}
G\brak{i, j, c, s} = \begin{bmatrix}
1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & \cdots & c & \cdots & s & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & \cdots & -\overline{s} & \cdots & \overline{c} & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & \cdots & 0 & \cdots & 1
\end{bmatrix}
\end{align}
where $c, s \in \mathbb{C}$ and $\abs{c}^2 + \abs{s}^2 = 1$.
We can see that $G$ is a unitary matrix. 
\newline
Say we take a vector $\defvec{x}{m}$, and $\vecb{y} = G\brak{i, j, c, s}\vecb{x}$, then
\begin{align}
    y_k = \begin{cases}
        c x_i - s x_j, & k = i \\
        s x_i + c x_j, & k = j \\
        x_k, & k \neq i, j
    \end{cases}
\end{align}

\subsubsection{Givens Rotation Algorithm}
\begin{enumerate}
    \item Identify the non-zero elements below the main diagonal.
    \item Compute the cosine and sine values using the elements of the matrix.
    \item Apply the Givens rotation matrix to zero out the sub-diagonal elements.
\end{enumerate}

\subsection{Gram-Schmidt Orthogonalization}
The Gram-Schmidt process is used to orthogonalize the columns of the matrix, converting it into a product of a unitary matrix $Q$ and an upper triangular matrix $R$.

\subsubsection{Algorithm}
\begin{enumerate}
    \item For each column vector $\mathbf{a}_i$, subtract its projection onto the previously computed orthogonal vectors.
    \item Normalize the resulting vector to form the orthogonal basis.
\end{enumerate}

\section{Implementation}
The following section provides an overview of the C code implementation. Key functions include:

\subsection{Matrix Operations}
\begin{itemize}
    \item \texttt{mzeroes()}: Creates a matrix filled with zeros.
    \item \texttt{meye()}: Returns the identity matrix.
    \item \texttt{mmul()}: Multiplies two matrices.
    \item \texttt{mT()}: Computes the transpose conjugate of a matrix.
\end{itemize}

\subsection{Householder Transformation Code}
\begin{lstlisting}[language=C, caption=Hessenberg Reduction]
compl** hess(compl** A, int m, double tolerance) {
    for (int i = 0; i < m - 2; i++) {
        compl** P_i = meye(m);
        compl** x = mzeroes(m - i - 1, 1);
        for (int k = i + 1; k < m; k++) x[k - i - 1][0] = A[k][i];

        compl rho = (x[0][0] == 0) ? 0 : -(x[0][0]) / cabs(x[0][0]);
        compl** u = madd(x, mscale(e(m - i - 1, 1), m - i - 1, 1, -rho * vnorm(x, m - i - 1)), m - i - 1, 1);

        if (vnorm(u, m - i - 1) > tolerance) u = mscale(u, m - i - 1, 1, 1 / vnorm(u, m - i - 1));
        compl** P_sub = madd(meye(m - i - 1), mscale(mmul(u, mT(u, m - i - 1, 1), m - i - 1, 1, m - i - 1), -2), m - i - 1, m - i - 1);

        for (int j = i + 1; j < m; j++)
            for (int k = i + 1; k < m; k++)
                P_i[j][k] = P_sub[j - i - 1][k - i - 1];

        A = mmul(A, P_i, m, m, m);
        A = mmul(P_i, A, m, m, m);
    }
    return A;
}
\end{lstlisting}

\subsection{Givens Rotations Code}
\begin{lstlisting}[language=C, caption=Givens Rotations]
compl** givens(compl** H, int m, double tolerance) {
    for (int i = 0; i < m - 1; i++) {
        compl** vec = mgetcol(H, m, m, i);
        compl** G = g_mat(m, i, i + 1, vec, tolerance);
        H = mmul(G, H, m, m, m);
        H = mmul(H, mT(G, m, m), m, m, m);
    }
    return H;
}
\end{lstlisting}

\section{Numerical Results}
We tested the implementation on various complex matrices. Table~\ref{tab:results} summarizes the results, comparing the computed eigenvalues with the expected results.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Matrix & Computed Eigenvalues & Expected Eigenvalues \\
        \hline
        $\begin{bmatrix} 2+i & 1 \\ 1-i & 3-i \end{bmatrix}$ & $3.56, 0.44$ & $3.56, 0.44$ \\
        \hline
    \end{tabular}
    \caption{Comparison of Eigenvalues for Test Matrices}
    \label{tab:results}
\end{table}

\section{Conclusion}
This report presented an efficient algorithm for finding eigenvalues of complex matrices using Householder transformations, Givens rotations, and the Gram-Schmidt process. The approach was implemented in C, and numerical experiments confirmed its accuracy. Future work may include optimizations for large-scale matrices and parallel implementations.

\section*{References}
\begin{enumerate}
    \item Golub, G. H., and Van Loan, C. F., \textit{Matrix Computations}, Johns Hopkins University Press, 2013.
    \item Trefethen, L. N., and Bau, D., \textit{Numerical Linear Algebra}, SIAM, 1997.
\end{enumerate}

\end{document}
